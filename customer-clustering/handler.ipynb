{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Return Clustering - Complete Pipeline Handler\n",
    "# Orchestrates the entire pipeline from data loading to clustering results\n",
    "\n",
    "**Execute each cell in order to run the complete customer clustering pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# Setup comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'pipeline_execution_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"ğŸ“¦ Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CUSTOMER RETURN CLUSTERING - COMPLETE PIPELINE\n",
      "ğŸ‰ ENHANCED WITH ALL 22 CSV COLUMNS!\n",
      "================================================================================\n",
      "Pipeline execution started at: 2025-06-25 00:28:45\n",
      "CSV File: base_returns_sku_reasoncodes_sent.csv\n",
      "Using 32 columns including bonus data!\n",
      "âœ… CSV file found: base_returns_sku_reasoncodes_sent.csv (715.4 MB)\n",
      "âœ¨ Bonus features available:\n",
      "   ğŸ“Š ORDERLINK - Enhanced clustering features possible!\n",
      "   ğŸ“Š Q_SKU_DESC - Enhanced clustering features possible!\n",
      "   ğŸ“Š Q_GMM_ID - Enhanced clustering features possible!\n",
      "   ğŸ“Š DIVISION_ - Enhanced clustering features possible!\n",
      "   ğŸ“Š Q_CLR_DNUM - Enhanced clustering features possible!\n",
      "   ğŸ“Š Q_CLR_DESC - Enhanced clustering features possible!\n",
      "   ğŸ“Š VENDOR_STYLE - Enhanced clustering features possible!\n",
      "   ğŸ“Š SIZE_ - Enhanced clustering features possible!\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE CONFIGURATION - UPDATE YOUR CSV FILE PATH HERE!\n",
    "\n",
    "# ENHANCED PIPELINE CONFIGURATION\n",
    "PIPELINE_CONFIG = {\n",
    "    'data_source': {\n",
    "        'csv_file': 'base_returns_sku_reasoncodes_sent.csv',  # Your new CSV!\n",
    "        'expected_columns': [\n",
    "            # All 31 columns including GROSS, SENTIMENT, SCORE, etc.\n",
    "            'SALES_ORDER_NO', 'CUSTOMER_EMAILID', 'ORDER_DATE', 'SKU', \n",
    "            'SALES_QTY', 'GROSS', 'RETURN_QTY', 'UNITS_RETURNED_FLAG', \n",
    "            'RETURN_DATE', 'RETURN_NO', 'RETURN_COMMENT', 'RETURN_REASON',\n",
    "            'ORDERLINK', 'MAX(RR.F_ID)', 'MAX(RR.F_NAME)', \n",
    "            'MAX(RR.Q_D2C_RTRN_REASONCODE_ID)', 'MAX(RR.Q_D2C_RET_REASON_NAME)',\n",
    "            'MAX(RR.Q_D2C_RET_REASON)', 'MAX(RR.Q_D2C_RET_REASON_GROUP_NAME0)',\n",
    "            'Q_CLS_ID', 'Q_SKU_DESC', 'Q_GMM_ID', 'Q_SKU_ID', 'CLASS_', \n",
    "            'DIVISION_', 'BRAND_', 'Q_CLR_DNUM', 'Q_CLR_DESC', \n",
    "            'VENDOR_STYLE', 'SIZE_', 'SENTIMENT', 'SCORE'\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    'execution_steps': {\n",
    "        'database_setup': False,\n",
    "        'data_analysis': True,\n",
    "        'feature_creation': True,\n",
    "        'preprocessing': True,\n",
    "        'clustering': False  # Set to True to run clustering automatically\n",
    "    },\n",
    "    \n",
    "    'error_handling': {\n",
    "        'continue_on_warning': True,\n",
    "        'stop_on_error': True,\n",
    "        'save_intermediate_results': True,\n",
    "        'max_retries': 2\n",
    "    },\n",
    "    \n",
    "    'quality_checks': {\n",
    "        'min_customers': 1000,\n",
    "        'min_orders_per_customer': 2,\n",
    "        'max_missing_data_ratio': 0.3,\n",
    "        'min_feature_completeness': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CUSTOMER RETURN CLUSTERING - COMPLETE PIPELINE\")\n",
    "print(\"ğŸ‰ ENHANCED WITH ALL 22 CSV COLUMNS!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Pipeline execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"CSV File: {PIPELINE_CONFIG['data_source']['csv_file']}\")\n",
    "print(f\"Using {len(PIPELINE_CONFIG['data_source']['expected_columns'])} columns including bonus data!\")\n",
    "\n",
    "# Check if CSV file exists\n",
    "csv_path = Path(PIPELINE_CONFIG['data_source']['csv_file'])\n",
    "if csv_path.exists():\n",
    "    print(f\"âœ… CSV file found: {csv_path} ({csv_path.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "    print(\"âœ¨ Bonus features available:\")\n",
    "    bonus_cols = ['ORDERLINK', 'Q_SKU_DESC', 'Q_GMM_ID', 'DIVISION_', \n",
    "                  'Q_CLR_DNUM', 'Q_CLR_DESC', 'VENDOR_STYLE', 'SIZE_']\n",
    "    for col in bonus_cols:\n",
    "        print(f\"   ğŸ“Š {col} - Enhanced clustering features possible!\")\n",
    "else:\n",
    "    print(f\"âŒ CSV file not found: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pipeline executor initialized!\n"
     ]
    }
   ],
   "source": [
    "# Pipeline execution utilities class\n",
    "\n",
    "class PipelineExecutor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.execution_log = []\n",
    "        self.start_time = time.time()\n",
    "        self.step_times = {}\n",
    "        \n",
    "    def log_step(self, step_name, status, duration=None, details=None):\n",
    "        \"\"\"Log pipeline step execution\"\"\"\n",
    "        log_entry = {\n",
    "            'step': step_name,\n",
    "            'status': status,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'duration_seconds': duration,\n",
    "            'details': details\n",
    "        }\n",
    "        self.execution_log.append(log_entry)\n",
    "        \n",
    "        if duration:\n",
    "            logger.info(f\"Step '{step_name}' {status.lower()} in {duration:.2f} seconds\")\n",
    "        else:\n",
    "            logger.info(f\"Step '{step_name}' {status.lower()}\")\n",
    "    \n",
    "    def run_python_module(self, module_name, description, *args):\n",
    "        \"\"\"Execute a Python module and handle errors\"\"\"\n",
    "        step_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Starting: {description}\")\n",
    "            \n",
    "            if module_name == 'db':\n",
    "                from db import setup_database  # âœ… Use enhanced version\n",
    "                db = setup_database(self.config['data_source']['csv_file'])\n",
    "                summary = db.get_data_summary()  # âœ… Enhanced summary\n",
    "                db.close()\n",
    "                result = summary\n",
    "                \n",
    "            elif module_name == 'create_features':\n",
    "                result = subprocess.run([\n",
    "                    sys.executable, 'create_features.py'\n",
    "                ], capture_output=True, text=True, check=True)\n",
    "                result = {'stdout': result.stdout, 'stderr': result.stderr}\n",
    "                \n",
    "            elif module_name == 'cluster_preprocessing':\n",
    "                result = subprocess.run([\n",
    "                    sys.executable, 'cluster_preprocessing.py'\n",
    "                ] + list(args), capture_output=True, text=True, check=True)\n",
    "                result = {'stdout': result.stdout, 'stderr': result.stderr}\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown module: {module_name}\")\n",
    "            \n",
    "            duration = time.time() - step_start\n",
    "            self.step_times[module_name] = duration\n",
    "            self.log_step(description, \"COMPLETED\", duration, result)\n",
    "            \n",
    "            return True, result\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            duration = time.time() - step_start\n",
    "            error_details = {\n",
    "                'error_type': 'subprocess_error',\n",
    "                'return_code': e.returncode,\n",
    "                'stdout': e.stdout,\n",
    "                'stderr': e.stderr\n",
    "            }\n",
    "            self.log_step(description, \"FAILED\", duration, error_details)\n",
    "            logger.error(f\"Subprocess failed: {e}\")\n",
    "            logger.error(f\"STDOUT: {e.stdout}\")\n",
    "            logger.error(f\"STDERR: {e.stderr}\")\n",
    "            \n",
    "            return False, error_details\n",
    "            \n",
    "        except Exception as e:\n",
    "            duration = time.time() - step_start\n",
    "            error_details = {\n",
    "                'error_type': 'execution_error',\n",
    "                'error_message': str(e),\n",
    "                'traceback': traceback.format_exc()\n",
    "            }\n",
    "            self.log_step(description, \"FAILED\", duration, error_details)\n",
    "            logger.error(f\"Execution failed: {e}\")\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            \n",
    "            return False, error_details\n",
    "    \n",
    "    def save_execution_summary(self):\n",
    "        \"\"\"Save comprehensive execution summary\"\"\"\n",
    "        total_duration = time.time() - self.start_time\n",
    "        \n",
    "        summary = {\n",
    "            'pipeline_execution': {\n",
    "                'start_time': datetime.fromtimestamp(self.start_time).isoformat(),\n",
    "                'end_time': datetime.now().isoformat(),\n",
    "                'total_duration_seconds': total_duration,\n",
    "                'total_duration_formatted': f\"{total_duration//3600:.0f}h {(total_duration%3600)//60:.0f}m {total_duration%60:.0f}s\"\n",
    "            },\n",
    "            'configuration': self.config,\n",
    "            'step_execution_log': self.execution_log,\n",
    "            'step_durations': self.step_times,\n",
    "            'status_summary': {\n",
    "                'total_steps': len(self.execution_log),\n",
    "                'completed_steps': len([log for log in self.execution_log if log['status'] == 'COMPLETED']),\n",
    "                'failed_steps': len([log for log in self.execution_log if log['status'] == 'FAILED'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to JSON\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        with open(f'pipeline_execution_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Execution summary saved to pipeline_execution_summary_{timestamp}.json\")\n",
    "        return summary\n",
    "\n",
    "# Initialize pipeline executor\n",
    "pipeline = PipelineExecutor(PIPELINE_CONFIG)\n",
    "print(\"âœ… Pipeline executor initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸ Database setup skipped (disabled in configuration)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: DATABASE SETUP AND DATA LOADING\n",
    "\n",
    "if PIPELINE_CONFIG['execution_steps']['database_setup']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: DATABASE SETUP AND DATA LOADING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if CSV file exists\n",
    "    csv_path = Path(PIPELINE_CONFIG['data_source']['csv_file'])\n",
    "    if not csv_path.exists():\n",
    "        print(f\"âŒ CSV file not found: {csv_path}\")\n",
    "        print(\"Please update the csv_file path in PIPELINE_CONFIG\")\n",
    "        pipeline.log_step(\"Database Setup\", \"FAILED\", details=\"CSV file not found\")\n",
    "    else:\n",
    "        print(f\"âœ… CSV file found: {csv_path} ({csv_path.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "        \n",
    "        # Execute database setup\n",
    "        success, result = pipeline.run_python_module('db', 'Database Setup and Data Loading')\n",
    "        \n",
    "        if success:\n",
    "            print(f\"\\nâœ… Database setup completed successfully!\")\n",
    "            print(f\"Data summary: {result}\")\n",
    "        else:\n",
    "            print(f\"âŒ Database setup failed: {result}\")\n",
    "            if PIPELINE_CONFIG['error_handling']['stop_on_error']:\n",
    "                print(\"Stopping pipeline due to error configuration\")\n",
    "                raise Exception(\"Database setup failed\")\n",
    "else:\n",
    "    print(\"â­ï¸ Database setup skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: DATA ANALYSIS\n",
      "============================================================\n",
      "Running basic data analysis...\n",
      "Analyzing customer distribution...\n",
      "2025-06-25 00:28:45,868 - INFO - 4185141521 - Step 'Data Analysis' completed in 0.17 seconds\n",
      "âœ… Data analysis completed in 0.17 seconds\n",
      "Analysis results:\n",
      "  total_customers: 14,999\n",
      "  avg_orders_per_customer: 67.407\n",
      "  avg_qty_per_customer: 259.890\n",
      "  avg_return_rate: 0.214\n",
      "  customers_with_sufficient_orders: 14,999\n",
      "âœ… 100.0% customers have sufficient order history\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: DATA ANALYSIS\n",
    "\n",
    "if PIPELINE_CONFIG['execution_steps']['data_analysis']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    step_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"Running basic data analysis...\")\n",
    "        \n",
    "        from db import get_connection\n",
    "        conn = get_connection(\"customer_clustering.db\")\n",
    "        \n",
    "        # Run key analyses\n",
    "        print(\"Analyzing customer distribution...\")\n",
    "        customer_analysis = conn.execute(\"\"\"\n",
    "            WITH customer_summary AS (\n",
    "                SELECT \n",
    "                    customer_emailid,\n",
    "                    count(DISTINCT sales_order_no) as order_count,\n",
    "                    count(*) as item_count,\n",
    "                    sum(sales_qty) as total_sales_qty,\n",
    "                    sum(return_qty) as total_return_qty,\n",
    "                    min(order_date) as first_order,\n",
    "                    max(order_date) as last_order\n",
    "                FROM bronze_return_order_data\n",
    "                GROUP BY customer_emailid\n",
    "            )\n",
    "            SELECT \n",
    "                count(*) as total_customers,\n",
    "                avg(order_count) as avg_orders_per_customer,\n",
    "                avg(total_sales_qty) as avg_qty_per_customer,\n",
    "                avg(CAST(total_return_qty AS DOUBLE) / CAST(total_sales_qty AS DOUBLE)) as avg_return_rate,\n",
    "                count(*) FILTER (WHERE order_count >= ?) as customers_with_sufficient_orders\n",
    "            FROM customer_summary;\n",
    "        \"\"\", [PIPELINE_CONFIG['quality_checks']['min_orders_per_customer']]).fetchone()\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        duration = time.time() - step_start\n",
    "        \n",
    "        analysis_results = {\n",
    "            'total_customers': customer_analysis[0],\n",
    "            'avg_orders_per_customer': customer_analysis[1],\n",
    "            'avg_qty_per_customer': customer_analysis[2],\n",
    "            'avg_return_rate': customer_analysis[3],\n",
    "            'customers_with_sufficient_orders': customer_analysis[4]\n",
    "        }\n",
    "        \n",
    "        pipeline.log_step(\"Data Analysis\", \"COMPLETED\", duration, analysis_results)\n",
    "        \n",
    "        print(f\"âœ… Data analysis completed in {duration:.2f} seconds\")\n",
    "        print(f\"Analysis results:\")\n",
    "        for key, value in analysis_results.items():\n",
    "            print(f\"  {key}: {value:,.3f}\" if isinstance(value, float) else f\"  {key}: {value:,}\")\n",
    "        \n",
    "        # Check if we have sufficient data quality\n",
    "        sufficient_customers_ratio = analysis_results['customers_with_sufficient_orders'] / analysis_results['total_customers']\n",
    "        if sufficient_customers_ratio < 0.5:\n",
    "            print(f\"âš ï¸ Warning: Only {sufficient_customers_ratio:.1%} customers have sufficient order history\")\n",
    "        else:\n",
    "            print(f\"âœ… {sufficient_customers_ratio:.1%} customers have sufficient order history\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        duration = time.time() - step_start\n",
    "        pipeline.log_step(\"Data Analysis\", \"FAILED\", duration, str(e))\n",
    "        print(f\"âŒ Data analysis failed: {e}\")\n",
    "        \n",
    "        if PIPELINE_CONFIG['error_handling']['stop_on_error']:\n",
    "            print(\"Stopping pipeline due to error configuration\")\n",
    "            raise\n",
    "else:\n",
    "    print(\"â­ï¸ Data analysis skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: FEATURE CREATION\n",
      "============================================================\n",
      "2025-06-25 01:11:49,828 - INFO - 4185141521 - Starting: Feature Engineering and Silver Layer Creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Code\\Local Code\\URBN\\RETRO_hack_25\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"C:\\Python312\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 253: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-25 01:12:40,430 - INFO - 4185141521 - Step 'Feature Engineering and Silver Layer Creation' completed in 50.60 seconds\n",
      "âœ… Feature creation completed successfully!\n",
      "Silver layer populated with customer features\n",
      "\n",
      "ğŸ“Š Created 33 features across 8 categories:\n",
      "  - Volume Metrics (5 features)\n",
      "  - Return Behavior (6 features)\n",
      "  - Temporal Patterns (3 features)\n",
      "  - Recency Analysis (4 features)\n",
      "  - Category Intelligence (3 features)\n",
      "  - Adjacency Patterns (4 features)\n",
      "  - Seasonal Trends (2 features)\n",
      "  - Trend Analysis (2 features)\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: FEATURE CREATION\n",
    "\n",
    "if PIPELINE_CONFIG['execution_steps']['feature_creation']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 3: FEATURE CREATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    success, result = pipeline.run_python_module('create_features', 'Feature Engineering and Silver Layer Creation')\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… Feature creation completed successfully!\")\n",
    "        print(\"Silver layer populated with customer features\")\n",
    "        print(\"\\nğŸ“Š Created 33 features across 8 categories:\")\n",
    "        print(\"  - Volume Metrics (5 features)\")\n",
    "        print(\"  - Return Behavior (6 features)\")\n",
    "        print(\"  - Temporal Patterns (3 features)\")\n",
    "        print(\"  - Recency Analysis (4 features)\")\n",
    "        print(\"  - Category Intelligence (3 features)\")\n",
    "        print(\"  - Adjacency Patterns (4 features)\")\n",
    "        print(\"  - Seasonal Trends (2 features)\")\n",
    "        print(\"  - Trend Analysis (2 features)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âŒ Feature creation failed\")\n",
    "        print(f\"Error details: {result}\")\n",
    "        \n",
    "        if PIPELINE_CONFIG['error_handling']['stop_on_error']:\n",
    "            print(\"Stopping pipeline due to error configuration\")\n",
    "            raise Exception(\"Feature creation failed\")\n",
    "else:\n",
    "    print(\"â­ï¸ Feature creation skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: CLUSTER PREPROCESSING\n",
      "============================================================\n",
      "2025-06-25 01:14:11,099 - INFO - 4185141521 - Starting: Cluster Preprocessing and Gold Layer Creation\n",
      "2025-06-25 01:14:17,094 - INFO - 4185141521 - Step 'Cluster Preprocessing and Gold Layer Creation' failed in 6.00 seconds\n",
      "2025-06-25 01:14:17,095 - ERROR - 4185141521 - Subprocess failed: Command '['c:\\\\Code\\\\Local Code\\\\URBN\\\\RETRO_hack_25\\\\.venv\\\\Scripts\\\\python.exe', 'cluster_preprocessing.py', '--scaling', 'robust', '--variance-threshold', '0.01', '--outlier-contamination', '0.05']' returned non-zero exit status 1.\n",
      "2025-06-25 01:14:17,096 - ERROR - 4185141521 - STDOUT: \n",
      "2025-06-25 01:14:17,096 - ERROR - 4185141521 - STDERR: 2025-06-25 01:14:15,894 - INFO - Starting cluster preprocessing pipeline...\n",
      "2025-06-25 01:14:15,894 - INFO - Parameters: scaling=robust, variance_threshold=0.01, contamination=0.05\n",
      "2025-06-25 01:14:15,913 - INFO - Connected to customer clustering database\n",
      "2025-06-25 01:14:15,913 - INFO - Silver layer contains 14,999 customer records\n",
      "2025-06-25 01:14:15,913 - INFO - Initialized ClusterPreprocessor with robust scaling\n",
      "2025-06-25 01:14:15,913 - INFO - Loading data from silver layer...\n",
      "2025-06-25 01:14:15,928 - INFO - Loaded 14999 customers with 44 features\n",
      "2025-06-25 01:14:15,928 - INFO - Selecting features for clustering...\n",
      "2025-06-25 01:14:15,928 - INFO - Including category_diversity_score: 99.8% customers have data\n",
      "2025-06-25 01:14:15,928 - INFO - Including category_loyalty_score: 100.0% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including high_return_category_affinity: 99.8% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_orders: 98.7% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_returns: 64.3% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_timing: 98.2% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_return_timing: 62.6% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including seasonal_susceptibility_orders: 85.9% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including seasonal_susceptibility_returns: 80.9% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including trend_product_category_order_rate: 99.6% customers have data\n",
      "2025-06-25 01:14:15,930 - INFO - Including trend_product_category_return_rate: 92.9% customers have data\n",
      "2025-06-25 01:14:15,932 - INFO - Selected 29 features for clustering:\n",
      "2025-06-25 01:14:15,932 - INFO -   volume_metrics: 5 features\n",
      "2025-06-25 01:14:15,932 - INFO -   return_behavior: 6 features\n",
      "2025-06-25 01:14:15,932 - INFO -   temporal_patterns: 3 features\n",
      "2025-06-25 01:14:15,932 - INFO -   recency_trends: 4 features\n",
      "2025-06-25 01:14:15,932 - INFO -   category_intelligence: 3 features\n",
      "2025-06-25 01:14:15,932 - INFO -   adjacency_patterns: 4 features\n",
      "2025-06-25 01:14:15,932 - INFO -   seasonal_trends: 2 features\n",
      "2025-06-25 01:14:15,932 - INFO -   trend_susceptibility: 2 features\n",
      "2025-06-25 01:14:15,932 - INFO - Handling missing values...\n",
      "2025-06-25 01:14:15,934 - INFO - Removing low variance features (threshold: 0.01)...\n",
      "2025-06-25 01:14:15,936 - INFO - Removed 1 low variance features:\n",
      "2025-06-25 01:14:15,936 - INFO -   category_loyalty_score: variance = 0.008292\n",
      "2025-06-25 01:14:15,937 - INFO - Detecting outliers using Isolation Forest (contamination: 0.05)...\n",
      "2025-06-25 01:14:16,210 - INFO - Detected 750 outliers (5.0%)\n",
      "2025-06-25 01:14:16,211 - INFO - Outlier characteristics:\n",
      "2025-06-25 01:14:16,211 - INFO -   Avg return rate: 0.393\n",
      "2025-06-25 01:14:16,211 - INFO -   Avg order count: 420.0\n",
      "2025-06-25 01:14:16,211 - INFO -   Avg lifetime days: 967\n",
      "2025-06-25 01:14:16,212 - INFO - Scaling features using robust scaler...\n",
      "2025-06-25 01:14:16,228 - INFO - Scaled 28 features\n",
      "2025-06-25 01:14:16,228 - INFO - Creating gold layer dataset...\n",
      "2025-06-25 01:14:16,228 - INFO - Calculating feature completeness scores...\n",
      "2025-06-25 01:14:16,782 - INFO - Average feature completeness: 0.000\n",
      "2025-06-25 01:14:16,792 - INFO - Gold layer dataset created: 14999 customers, 35 columns\n",
      "2025-06-25 01:14:16,792 - INFO - Inserting data into gold_cluster_processed table...\n",
      "2025-06-25 01:14:16,793 - INFO - Cleared existing gold layer data\n",
      "2025-06-25 01:14:16,805 - ERROR - Failed to insert gold layer data: Binder Error: table gold_cluster_processed has 41 columns but 34 values were supplied\n",
      "2025-06-25 01:14:16,805 - ERROR - Failed to insert data into gold layer\n",
      "\n",
      "âŒ Cluster preprocessing failed\n",
      "Error details: {'error_type': 'subprocess_error', 'return_code': 1, 'stdout': '', 'stderr': '2025-06-25 01:14:15,894 - INFO - Starting cluster preprocessing pipeline...\\n2025-06-25 01:14:15,894 - INFO - Parameters: scaling=robust, variance_threshold=0.01, contamination=0.05\\n2025-06-25 01:14:15,913 - INFO - Connected to customer clustering database\\n2025-06-25 01:14:15,913 - INFO - Silver layer contains 14,999 customer records\\n2025-06-25 01:14:15,913 - INFO - Initialized ClusterPreprocessor with robust scaling\\n2025-06-25 01:14:15,913 - INFO - Loading data from silver layer...\\n2025-06-25 01:14:15,928 - INFO - Loaded 14999 customers with 44 features\\n2025-06-25 01:14:15,928 - INFO - Selecting features for clustering...\\n2025-06-25 01:14:15,928 - INFO - Including category_diversity_score: 99.8% customers have data\\n2025-06-25 01:14:15,928 - INFO - Including category_loyalty_score: 100.0% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including high_return_category_affinity: 99.8% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_orders: 98.7% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_returns: 64.3% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_timing: 98.2% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including sku_adjacency_return_timing: 62.6% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including seasonal_susceptibility_orders: 85.9% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including seasonal_susceptibility_returns: 80.9% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including trend_product_category_order_rate: 99.6% customers have data\\n2025-06-25 01:14:15,930 - INFO - Including trend_product_category_return_rate: 92.9% customers have data\\n2025-06-25 01:14:15,932 - INFO - Selected 29 features for clustering:\\n2025-06-25 01:14:15,932 - INFO -   volume_metrics: 5 features\\n2025-06-25 01:14:15,932 - INFO -   return_behavior: 6 features\\n2025-06-25 01:14:15,932 - INFO -   temporal_patterns: 3 features\\n2025-06-25 01:14:15,932 - INFO -   recency_trends: 4 features\\n2025-06-25 01:14:15,932 - INFO -   category_intelligence: 3 features\\n2025-06-25 01:14:15,932 - INFO -   adjacency_patterns: 4 features\\n2025-06-25 01:14:15,932 - INFO -   seasonal_trends: 2 features\\n2025-06-25 01:14:15,932 - INFO -   trend_susceptibility: 2 features\\n2025-06-25 01:14:15,932 - INFO - Handling missing values...\\n2025-06-25 01:14:15,934 - INFO - Removing low variance features (threshold: 0.01)...\\n2025-06-25 01:14:15,936 - INFO - Removed 1 low variance features:\\n2025-06-25 01:14:15,936 - INFO -   category_loyalty_score: variance = 0.008292\\n2025-06-25 01:14:15,937 - INFO - Detecting outliers using Isolation Forest (contamination: 0.05)...\\n2025-06-25 01:14:16,210 - INFO - Detected 750 outliers (5.0%)\\n2025-06-25 01:14:16,211 - INFO - Outlier characteristics:\\n2025-06-25 01:14:16,211 - INFO -   Avg return rate: 0.393\\n2025-06-25 01:14:16,211 - INFO -   Avg order count: 420.0\\n2025-06-25 01:14:16,211 - INFO -   Avg lifetime days: 967\\n2025-06-25 01:14:16,212 - INFO - Scaling features using robust scaler...\\n2025-06-25 01:14:16,228 - INFO - Scaled 28 features\\n2025-06-25 01:14:16,228 - INFO - Creating gold layer dataset...\\n2025-06-25 01:14:16,228 - INFO - Calculating feature completeness scores...\\n2025-06-25 01:14:16,782 - INFO - Average feature completeness: 0.000\\n2025-06-25 01:14:16,792 - INFO - Gold layer dataset created: 14999 customers, 35 columns\\n2025-06-25 01:14:16,792 - INFO - Inserting data into gold_cluster_processed table...\\n2025-06-25 01:14:16,793 - INFO - Cleared existing gold layer data\\n2025-06-25 01:14:16,805 - ERROR - Failed to insert gold layer data: Binder Error: table gold_cluster_processed has 41 columns but 34 values were supplied\\n2025-06-25 01:14:16,805 - ERROR - Failed to insert data into gold layer\\n'}\n",
      "Stopping pipeline due to error configuration\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Cluster preprocessing failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m PIPELINE_CONFIG[\u001b[33m'\u001b[39m\u001b[33merror_handling\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mstop_on_error\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     31\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStopping pipeline due to error configuration\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCluster preprocessing failed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâ­ï¸ Cluster preprocessing skipped (disabled in configuration)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Cluster preprocessing failed"
     ]
    }
   ],
   "source": [
    "# STEP 4: CLUSTER PREPROCESSING\n",
    "\n",
    "if PIPELINE_CONFIG['execution_steps']['preprocessing']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 4: CLUSTER PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Set preprocessing parameters\n",
    "    preprocessing_args = [\n",
    "        '--scaling', 'robust',\n",
    "        '--variance-threshold', '0.01',\n",
    "        '--outlier-contamination', '0.05'\n",
    "    ]\n",
    "    \n",
    "    success, result = pipeline.run_python_module('cluster_preprocessing', 'Cluster Preprocessing and Gold Layer Creation', *preprocessing_args)\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… Cluster preprocessing completed successfully!\")\n",
    "        print(\"Gold layer ready for clustering\")\n",
    "        print(\"\\nğŸ¯ Preprocessing completed:\")\n",
    "        print(\"  - Feature scaling (RobustScaler)\")\n",
    "        print(\"  - Outlier detection (Isolation Forest)\")\n",
    "        print(\"  - Low variance feature removal\")\n",
    "        print(\"  - Data quality validation\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âŒ Cluster preprocessing failed\")\n",
    "        print(f\"Error details: {result}\")\n",
    "        \n",
    "        if PIPELINE_CONFIG['error_handling']['stop_on_error']:\n",
    "            print(\"Stopping pipeline due to error configuration\")\n",
    "            raise Exception(\"Cluster preprocessing failed\")\n",
    "else:\n",
    "    print(\"â­ï¸ Cluster preprocessing skipped (disabled in configuration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: CLUSTERING (OPTIONAL)\n",
    "\n",
    "if PIPELINE_CONFIG['execution_steps']['clustering']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: CLUSTERING EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"ğŸ”„ Automated clustering would run here...\")\n",
    "    print(\"ğŸ““ For now, please run create_clusters.ipynb manually for interactive analysis\")\n",
    "    pipeline.log_step(\"Clustering\", \"SKIPPED\", details=\"Manual execution required\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 5: CLUSTERING (SKIPPED)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"â„¹ï¸ Clustering execution is disabled in configuration\")\n",
    "    print(\"ğŸ““ To run clustering, execute the create_clusters.ipynb notebook\")\n",
    "    print(\"\\nğŸ¯ Next step: Open and run create_clusters.ipynb\")\n",
    "    pipeline.log_step(\"Clustering\", \"SKIPPED\", details=\"Disabled in configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE COMPLETION AND SUMMARY\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save execution summary\n",
    "execution_summary = pipeline.save_execution_summary()\n",
    "\n",
    "# Print summary\n",
    "total_time = execution_summary['pipeline_execution']['total_duration_formatted']\n",
    "completed_steps = execution_summary['status_summary']['completed_steps']\n",
    "total_steps = execution_summary['status_summary']['total_steps']\n",
    "failed_steps = execution_summary['status_summary']['failed_steps']\n",
    "\n",
    "print(f\"Execution completed in: {total_time}\")\n",
    "print(f\"Steps completed: {completed_steps}/{total_steps}\")\n",
    "\n",
    "if failed_steps > 0:\n",
    "    print(f\"âŒ {failed_steps} step(s) failed - check logs for details\")\n",
    "    print(\"\\nFailed steps:\")\n",
    "    for log_entry in execution_summary['step_execution_log']:\n",
    "        if log_entry['status'] == 'FAILED':\n",
    "            print(f\"  - {log_entry['step']}: {log_entry.get('details', {}).get('error_message', 'Unknown error')}\")\n",
    "else:\n",
    "    print(\"âœ… All enabled steps completed successfully!\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ NEXT STEPS:\")\n",
    "if failed_steps == 0:\n",
    "    if not PIPELINE_CONFIG['execution_steps']['clustering']:\n",
    "        print(\"  1. ğŸ““ Execute create_clusters.ipynb for clustering analysis\")\n",
    "        print(\"  2. ğŸ“Š Generate Excel report with: python generate_excel_report.py\")\n",
    "        print(\"  3. ğŸ’¼ Review clustering results and business interpretations\")\n",
    "        print(\"  4. ğŸš€ Implement customer segmentation in business systems\")\n",
    "    else:\n",
    "        print(\"  1. ğŸ“Š Generate Excel report with: python generate_excel_report.py\")\n",
    "        print(\"  2. ğŸ’¼ Review clustering results in database tables\")\n",
    "        print(\"  3. âœ… Validate customer segments with business stakeholders\")\n",
    "        print(\"  4. ğŸš€ Implement customer segmentation strategies\")\n",
    "else:\n",
    "    print(\"  1. ğŸ” Review error logs and fix any data quality issues\")\n",
    "    print(\"  2. ğŸ”„ Re-run failed pipeline steps\")\n",
    "    print(\"  3. âœ… Proceed with clustering once all steps complete successfully\")\n",
    "\n",
    "print(\"\\nğŸ“Š DATABASE TABLES CREATED:\")\n",
    "print(\"  - bronze_return_order_data (raw data)\")\n",
    "print(\"  - silver_customer_features (engineered features)\")\n",
    "print(\"  - gold_cluster_processed (clustering-ready data)\")\n",
    "if PIPELINE_CONFIG['execution_steps']['clustering']:\n",
    "    print(\"  - clustering_results (final customer segments)\")\n",
    "    print(\"  - cluster_summary (segment characteristics)\")\n",
    "\n",
    "print(\"\\nğŸ¯ BUSINESS VALUE:\")\n",
    "print(\"  - Customer segmentation based on return behavior patterns\")\n",
    "print(\"  - Identification of high-risk return customers\")\n",
    "print(\"  - Data-driven insights for retention and satisfaction strategies\")\n",
    "print(\"  - Foundation for personalized customer experiences\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Pipeline execution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show files created\n",
    "print(\"\\nğŸ“ FILES CREATED:\")\n",
    "files_created = [\n",
    "    'customer_clustering.db',\n",
    "    f'pipeline_execution_{datetime.now().strftime(\"%Y%m%d\")}_*.log',\n",
    "    f'pipeline_execution_summary_{datetime.now().strftime(\"%Y%m%d\")}_*.json'\n",
    "]\n",
    "\n",
    "for file_pattern in files_created:\n",
    "    print(f\"  âœ… {file_pattern}\")\n",
    "\n",
    "print(\"\\nğŸ‰ Ready for clustering analysis!\")\n",
    "print(\"ğŸ““ Next: Open and run create_clusters.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
