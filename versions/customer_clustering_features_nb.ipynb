{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Return Clustering Feature Engineering\n",
    "\n",
    "This notebook creates comprehensive customer features for DBSCAN clustering analysis.\n",
    "Designed to handle large datasets (7M+ rows, 25GB+) with efficient DuckDB processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Union, Optional\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "class PrependFileHandler(logging.Handler):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "\n",
    "    def emit(self, record):\n",
    "        log_entry = self.format(record)\n",
    "        try:\n",
    "            with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "                old_content = f.read()\n",
    "        except FileNotFoundError:\n",
    "            old_content = ''\n",
    "        with open(self.filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(log_entry + '\\n' + old_content)\n",
    "\n",
    "# Custom formatter for hh:mm:dd::mm:yy\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    def formatTime(self, record, datefmt=None):\n",
    "        ct = self.converter(record.created)\n",
    "        return f\"{ct.tm_hour:02}:{ct.tm_min:02}:{ct.tm_mday:02}::{ct.tm_mon:02}:{ct.tm_year % 100:02}\"\n",
    "\n",
    "# Configure logging\n",
    "log_file = 'log.txt'\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "# Remove all handlers\n",
    "logger.handlers = []\n",
    "prepend_handler = PrependFileHandler(log_file)\n",
    "formatter = CustomFormatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "prepend_handler.setFormatter(formatter)\n",
    "logger.addHandler(prepend_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_clustering_features(\n",
    "    file_path: \"data\\base_returns_sku_metadata.csv\",\n",
    "    table_name: str = 'customer_data',\n",
    "    features_table_name: str = 'customer_features',\n",
    "    chunk_size: int = 100000,\n",
    "    db_file: str = 'returns_v2.duckdb',\n",
    "    force_recreate: bool = False\n",
    ") -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Create comprehensive customer clustering features from sales/returns data.\n",
    "    \n",
    "    Optimized for large datasets (7M+ rows) with chunked processing and efficient SQL.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str or Path\n",
    "        Path to .xlsx or .csv file\n",
    "    table_name : str\n",
    "        Name for raw data table in DuckDB\n",
    "    features_table_name : str\n",
    "        Name for features table in DuckDB\n",
    "    chunk_size : int\n",
    "        Rows per chunk for memory management (default 100k)\n",
    "    db_file : str\n",
    "        DuckDB file path (':memory:' for in-memory)\n",
    "    force_recreate : bool\n",
    "        Whether to recreate existing tables\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    duckdb.DuckDBPyConnection\n",
    "        Connected DuckDB instance with features table\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize DuckDB connection\n",
    "    conn = duckdb.connect(db_file)\n",
    "    \n",
    "    # Configure DuckDB for large datasets\n",
    "    conn.execute(\"SET memory_limit='32GB'\")\n",
    "    conn.execute(\"SET threads=16\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and prepare raw data\n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        _load_raw_data(conn, file_path, table_name, chunk_size, force_recreate)\n",
    "        \n",
    "        # Step 2: Create feature engineering tables\n",
    "        logger.info(\"Creating intermediate feature tables\")\n",
    "        _create_intermediate_tables(conn, table_name)\n",
    "        \n",
    "        # Step 3: Generate all customer features\n",
    "        logger.info(\"Generating customer clustering features\")\n",
    "        _create_customer_features(conn, features_table_name, force_recreate)\n",
    "        \n",
    "        # Step 4: Validate and optimize\n",
    "        logger.info(\"Validating feature creation\")\n",
    "        _validate_features(conn, features_table_name)\n",
    "        \n",
    "        return conn\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in feature creation: {str(e)}\")\n",
    "        conn.close()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_raw_data(conn, file_path, table_name, chunk_size, force_recreate):\n",
    "    \"\"\"Load raw data into DuckDB with chunked processing for large files.\"\"\"\n",
    "    \n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_exists = conn.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM information_schema.tables \n",
    "        WHERE table_name = '{table_name}'\n",
    "    \"\"\").fetchone()[0] > 0\n",
    "    \n",
    "    if table_exists and not force_recreate:\n",
    "        logger.info(f\"Table {table_name} already exists, skipping data load\")\n",
    "        return\n",
    "    \n",
    "    # Drop table if force recreate\n",
    "    if table_exists and force_recreate:\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    \n",
    "    # Load data based on file type\n",
    "    if file_path.suffix.lower() == '.xlsx':\n",
    "        _load_excel_chunked(conn, file_path, table_name, chunk_size)\n",
    "    elif file_path.suffix.lower() == '.csv':\n",
    "        _load_csv_direct(conn, file_path, table_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_path.suffix}\")\n",
    "    \n",
    "    # Create indexes for performance\n",
    "    logger.info(\"Creating indexes on raw data table\")\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_customer ON {table_name}(CUSTOMER_EMAILID)\")\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_order ON {table_name}(SALES_ORDER_NO)\")\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_date ON {table_name}(ORDER_DATE)\")\n",
    "    conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_return ON {table_name}(UNITS_RETURNED_FLAG)\")\n",
    "\n",
    "\n",
    "def _load_excel_chunked(conn, file_path, table_name, chunk_size):\n",
    "    \"\"\"Load Excel file in chunks to manage memory.\"\"\"\n",
    "    \n",
    "    # Read Excel file in chunks\n",
    "    logger.info(f\"Reading Excel file in chunks of {chunk_size:,} rows\")\n",
    "    \n",
    "    # First, get total rows for progress tracking\n",
    "    df_sample = pd.read_excel(file_path, nrows=1)\n",
    "    total_rows = None  # Excel doesn't easily give total rows without reading all\n",
    "    \n",
    "    chunk_num = 0\n",
    "    table_created = False\n",
    "    \n",
    "    # Process in chunks\n",
    "    for chunk in pd.read_excel(file_path, chunksize=chunk_size):\n",
    "        chunk_num += 1\n",
    "        logger.info(f\"Processing chunk {chunk_num} ({len(chunk):,} rows)\")\n",
    "        \n",
    "        # Clean and prepare chunk\n",
    "        chunk = _clean_raw_data_chunk(chunk)\n",
    "        \n",
    "        # Create table on first chunk\n",
    "        if not table_created:\n",
    "            conn.execute(f\"CREATE TABLE {table_name} AS SELECT * FROM chunk\")\n",
    "            table_created = True\n",
    "        else:\n",
    "            conn.execute(f\"INSERT INTO {table_name} SELECT * FROM chunk\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def _load_csv_direct(conn, file_path, table_name):\n",
    "    \"\"\"Load CSV file directly using DuckDB's efficient CSV reader.\"\"\"\n",
    "    \n",
    "    logger.info(\"Loading CSV using DuckDB's native CSV reader\")\n",
    "    \n",
    "    # Use DuckDB's efficient CSV reading\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE TABLE {table_name} AS \n",
    "        SELECT \n",
    "            CUSTOMER_EMAILID,\n",
    "            SALES_ORDER_NO,\n",
    "            Q_GMM_ID,\n",
    "            Q_CLS_ID AS CATEGORY_ID,\n",
    "            SKU,\n",
    "            Q_SKU_DESC AS PRODUCT_NAME,\n",
    "            SALES_QTY,\n",
    "            UNITS_RETURNED_FLAG,\n",
    "            RETURN_NO,\n",
    "            RETURN_QTY,\n",
    "            CAST(ORDER_DATE AS TIMESTAMP) AS ORDER_DATE,\n",
    "            CASE \n",
    "                WHEN RETURN_DATE = '-' OR RETURN_DATE IS NULL THEN NULL\n",
    "                ELSE CAST(RETURN_DATE AS TIMESTAMP)\n",
    "            END AS RETURN_DATE\n",
    "        FROM read_csv_auto('{file_path}')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def _clean_raw_data_chunk(df):\n",
    "    \"\"\"Clean and standardize a data chunk.\"\"\"\n",
    "    \n",
    "    # Rename columns to standard names\n",
    "    df = df.rename(columns={\n",
    "        'Q_CLS_ID': 'CATEGORY_ID',\n",
    "        'Q_SKU_DESC': 'PRODUCT_NAME'\n",
    "    })\n",
    "    \n",
    "    # Clean return dates (convert '-' to None)\n",
    "    df['RETURN_DATE'] = df['RETURN_DATE'].replace('-', None)\n",
    "    \n",
    "    # Ensure date columns are datetime\n",
    "    df['ORDER_DATE'] = pd.to_datetime(df['ORDER_DATE'])\n",
    "    df['RETURN_DATE'] = pd.to_datetime(df['RETURN_DATE'], errors='coerce')\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    df['SALES_QTY'] = pd.to_numeric(df['SALES_QTY'], errors='coerce').fillna(0)\n",
    "    df['RETURN_QTY'] = pd.to_numeric(df['RETURN_QTY'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Standardize return flag\n",
    "    df['UNITS_RETURNED_FLAG'] = df['UNITS_RETURNED_FLAG'].str.upper().eq('YES')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_intermediate_tables(conn, table_name):\n",
    "    \"\"\"Create intermediate tables for efficient feature calculation.\"\"\"\n",
    "    \n",
    "    # Get reference date (newest order date)\n",
    "    reference_date = conn.execute(f\"SELECT MAX(ORDER_DATE) FROM {table_name}\").fetchone()[0]\n",
    "    logger.info(f\"Using reference date: {reference_date}\")\n",
    "    \n",
    "    # Create customer order summary\n",
    "    logger.info(\"Creating customer order summary\")\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE customer_order_summary AS\n",
    "        SELECT \n",
    "            CUSTOMER_EMAILID,\n",
    "            SALES_ORDER_NO,\n",
    "            ORDER_DATE,\n",
    "            COUNT(*) as items_in_order,\n",
    "            SUM(SALES_QTY) as total_qty_ordered,\n",
    "            COUNT(DISTINCT SKU) as unique_skus_in_order,\n",
    "            COUNT(DISTINCT CATEGORY_ID) as unique_categories_in_order,\n",
    "            SUM(CASE WHEN UNITS_RETURNED_FLAG THEN 1 ELSE 0 END) as items_returned_in_order,\n",
    "            SUM(CASE WHEN UNITS_RETURNED_FLAG THEN RETURN_QTY ELSE 0 END) as qty_returned_in_order\n",
    "            -- TODO: Add order value calculations when price data available\n",
    "            -- SUM(SALES_QTY * UNIT_PRICE) as order_value,\n",
    "            -- SUM(CASE WHEN UNITS_RETURNED_FLAG THEN RETURN_QTY * UNIT_PRICE ELSE 0 END) as return_value\n",
    "        FROM {table_name}\n",
    "        GROUP BY CUSTOMER_EMAILID, SALES_ORDER_NO, ORDER_DATE\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create customer item-level summary\n",
    "    logger.info(\"Creating customer item summary\")\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE customer_item_summary AS\n",
    "        SELECT \n",
    "            CUSTOMER_EMAILID,\n",
    "            SKU,\n",
    "            CATEGORY_ID,\n",
    "            COUNT(*) as purchase_frequency,\n",
    "            SUM(SALES_QTY) as total_qty_purchased,\n",
    "            SUM(CASE WHEN UNITS_RETURNED_FLAG THEN RETURN_QTY ELSE 0 END) as total_qty_returned,\n",
    "            COUNT(CASE WHEN UNITS_RETURNED_FLAG THEN 1 END) as return_frequency,\n",
    "            MIN(ORDER_DATE) as first_purchase_date,\n",
    "            MAX(ORDER_DATE) as last_purchase_date,\n",
    "            AVG(CASE WHEN UNITS_RETURNED_FLAG AND RETURN_DATE IS NOT NULL \n",
    "                     THEN EXTRACT(DAY FROM (RETURN_DATE - ORDER_DATE)) END) as avg_days_to_return\n",
    "        FROM {table_name}\n",
    "        GROUP BY CUSTOMER_EMAILID, SKU, CATEGORY_ID\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create return timing analysis\n",
    "    logger.info(\"Creating return timing analysis\")\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE return_timing_analysis AS\n",
    "        SELECT \n",
    "            CUSTOMER_EMAILID,\n",
    "            SALES_ORDER_NO,\n",
    "            SKU,\n",
    "            ORDER_DATE,\n",
    "            RETURN_DATE,\n",
    "            EXTRACT(DAY FROM (RETURN_DATE - ORDER_DATE)) as days_to_return,\n",
    "            EXTRACT(MONTH FROM ORDER_DATE) as order_month,\n",
    "            EXTRACT(MONTH FROM RETURN_DATE) as return_month\n",
    "        FROM {table_name}\n",
    "        WHERE UNITS_RETURNED_FLAG AND RETURN_DATE IS NOT NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    # Store reference date for feature calculations\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE reference_metadata AS\n",
    "        SELECT \n",
    "            '{reference_date}'::TIMESTAMP as reference_date,\n",
    "            '{reference_date}'::TIMESTAMP - INTERVAL '90 days' as recent_cutoff_date\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_customer_features(conn, features_table_name, force_recreate):\n",
    "    \"\"\"Create comprehensive customer clustering features using SQL.\"\"\"\n",
    "    \n",
    "    # Check if features table exists\n",
    "    table_exists = conn.execute(f\"\"\"\n",
    "        SELECT COUNT(*) FROM information_schema.tables \n",
    "        WHERE table_name = '{features_table_name}'\n",
    "    \"\"\").fetchone()[0] > 0\n",
    "    \n",
    "    if table_exists and not force_recreate:\n",
    "        logger.info(f\"Features table {features_table_name} already exists, skipping creation\")\n",
    "        return\n",
    "    \n",
    "    if table_exists and force_recreate:\n",
    "        conn.execute(f\"DROP TABLE IF EXISTS {features_table_name}\")\n",
    "    \n",
    "    logger.info(\"Creating comprehensive customer features\")\n",
    "    \n",
    "    # Create the main features table with all categories\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE TABLE {features_table_name} AS\n",
    "        WITH customer_base AS (\n",
    "            SELECT DISTINCT CUSTOMER_EMAILID\n",
    "            FROM customer_order_summary\n",
    "        ),\n",
    "        \n",
    "        -- üìä BASIC VOLUME METRICS\n",
    "        basic_metrics AS (\n",
    "            SELECT \n",
    "                cb.CUSTOMER_EMAILID,\n",
    "                \n",
    "                -- Core volume metrics\n",
    "                COALESCE(COUNT(DISTINCT cos.SALES_ORDER_NO), 0) as SALES_ORDER_NO_nunique,\n",
    "                COALESCE(COUNT(DISTINCT cis.SKU), 0) as SKU_nunique,\n",
    "                COALESCE(SUM(cis.return_frequency), 0) as ITEMS_RETURNED_COUNT,\n",
    "                COALESCE(AVG(cis.total_qty_purchased), 0) as SALES_QTY_mean,\n",
    "                COALESCE(AVG(cos.items_in_order), 0) as AVG_ORDER_SIZE\n",
    "                \n",
    "            FROM customer_base cb\n",
    "            LEFT JOIN customer_order_summary cos ON cb.CUSTOMER_EMAILID = cos.CUSTOMER_EMAILID\n",
    "            LEFT JOIN customer_item_summary cis ON cb.CUSTOMER_EMAILID = cis.CUSTOMER_EMAILID\n",
    "            GROUP BY cb.CUSTOMER_EMAILID\n",
    "        ),\n",
    "        \n",
    "        -- üîÑ RETURN BEHAVIOR PATTERNS  \n",
    "        return_behavior AS (\n",
    "            SELECT \n",
    "                cb.CUSTOMER_EMAILID,\n",
    "                \n",
    "                -- Return rates and ratios\n",
    "                COALESCE(\n",
    "                    SUM(cis.return_frequency)::FLOAT / NULLIF(COUNT(cis.SKU), 0), 0\n",
    "                ) as RETURN_RATE,\n",
    "                \n",
    "                COALESCE(\n",
    "                    SUM(cis.total_qty_returned)::FLOAT / NULLIF(SUM(cis.total_qty_purchased), 0), 0\n",
    "                ) as RETURN_RATIO,\n",
    "                \n",
    "                COALESCE(COUNT(DISTINCT CASE WHEN cis.return_frequency > 0 THEN cis.SKU END), 0) as RETURN_PRODUCT_VARIETY,\n",
    "                \n",
    "                COALESCE(\n",
    "                    SUM(cos.items_returned_in_order)::FLOAT / NULLIF(COUNT(DISTINCT cos.SALES_ORDER_NO), 0), 0\n",
    "                ) as AVG_RETURNS_PER_ORDER,\n",
    "                \n",
    "                COALESCE(\n",
    "                    SUM(CASE WHEN cos.items_returned_in_order > 0 THEN 1 ELSE 0 END)::FLOAT / \n",
    "                    NULLIF(COUNT(DISTINCT cos.SALES_ORDER_NO), 0), 0\n",
    "                ) as RETURN_FREQUENCY_RATIO,\n",
    "                \n",
    "                COALESCE(\n",
    "                    AVG(CASE WHEN cis.total_qty_purchased > 0 \n",
    "                        THEN cis.total_qty_returned::FLOAT / cis.total_qty_purchased END), 0\n",
    "                ) as RETURN_INTENSITY,\n",
    "                \n",
    "                -- Consecutive returns pattern\n",
    "                COALESCE(_calculate_consecutive_returns(cb.CUSTOMER_EMAILID), 0) as CONSECUTIVE_RETURNS,\n",
    "                COALESCE(_calculate_avg_consecutive_returns(cb.CUSTOMER_EMAILID), 0) as AVG_CONSECUTIVE_RETURNS\n",
    "                \n",
    "            FROM customer_base cb\n",
    "            LEFT JOIN customer_order_summary cos ON cb.CUSTOMER_EMAILID = cos.CUSTOMER_EMAILID\n",
    "            LEFT JOIN customer_item_summary cis ON cb.CUSTOMER_EMAILID = cis.CUSTOMER_EMAILID\n",
    "            GROUP BY cb.CUSTOMER_EMAILID\n",
    "        ),\n",
    "        \n",
    "        -- ‚è∞ TEMPORAL & TIMING PATTERNS\n",
    "        temporal_patterns AS (\n",
    "            SELECT \n",
    "                cb.CUSTOMER_EMAILID,\n",
    "                \n",
    "                COALESCE(\n",
    "                    EXTRACT(DAY FROM (MAX(cos.ORDER_DATE) - MIN(cos.ORDER_DATE))), 0\n",
    "                ) as CUSTOMER_LIFETIME_DAYS,\n",
    "                \n",
    "                COALESCE(AVG(rta.days_to_return), 0) as AVG_DAYS_TO_RETURN,\n",
    "                \n",
    "                COALESCE(STDDEV(rta.days_to_return), 0) as RETURN_TIMING_SPREAD,\n",
    "                \n",
    "                CASE \n",
    "                    WHEN EXTRACT(DAY FROM (MAX(cos.ORDER_DATE) - MIN(cos.ORDER_DATE))) <= 90 THEN 'New'\n",
    "                    WHEN EXTRACT(DAY FROM (MAX(cos.ORDER_DATE) - MIN(cos.ORDER_DATE))) <= 180 THEN 'Growing' \n",
    "                    WHEN EXTRACT(DAY FROM (MAX(cos.ORDER_DATE) - MIN(cos.ORDER_DATE))) <= 365 THEN 'Mature'\n",
    "                    ELSE 'Established'\n",
    "                END as CUSTOMER_TENURE_STAGE\n",
    "                \n",
    "            FROM customer_base cb\n",
    "            LEFT JOIN customer_order_summary cos ON cb.CUSTOMER_EMAILID = cos.CUSTOMER_EMAILID\n",
    "            LEFT JOIN return_timing_analysis rta ON cb.CUSTOMER_EMAILID = rta.CUSTOMER_EMAILID\n",
    "            GROUP BY cb.CUSTOMER_EMAILID\n",
    "        ),\n",
    "        \n",
    "        -- üìà TREND & RECENCY ANALYSIS\n",
    "        trend_recency AS (\n",
    "            SELECT \n",
    "                cb.CUSTOMER_EMAILID,\n",
    "                \n",
    "                COALESCE(\n",
    "                    COUNT(DISTINCT CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date \n",
    "                                       THEN cos.SALES_ORDER_NO END), 0\n",
    "                ) as RECENT_ORDERS,\n",
    "                \n",
    "                COALESCE(\n",
    "                    SUM(CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date \n",
    "                             THEN cos.items_returned_in_order ELSE 0 END), 0\n",
    "                ) as RECENT_RETURNS,\n",
    "                \n",
    "                -- Recent vs average ratio (trend indicator)\n",
    "                CASE \n",
    "                    WHEN COUNT(DISTINCT cos.SALES_ORDER_NO) > 0 AND \n",
    "                         COUNT(DISTINCT CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date THEN cos.SALES_ORDER_NO END) > 0\n",
    "                    THEN (\n",
    "                        SUM(CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date THEN cos.items_returned_in_order ELSE 0 END)::FLOAT /\n",
    "                        NULLIF(COUNT(DISTINCT CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date THEN cos.SALES_ORDER_NO END), 0)\n",
    "                    ) / NULLIF(\n",
    "                        SUM(cos.items_returned_in_order)::FLOAT / COUNT(DISTINCT cos.SALES_ORDER_NO), 0\n",
    "                    )\n",
    "                    ELSE 1.0\n",
    "                END as RECENT_VS_AVG_RATIO,\n",
    "                \n",
    "                -- TODO: Add trend calculations for order and return frequency\n",
    "                1.0 as ORDER_FREQUENCY_TREND,  -- Placeholder\n",
    "                1.0 as RETURN_FREQUENCY_TREND, -- Placeholder\n",
    "                \n",
    "                -- Behavior stability (consistency of recent vs historical)\n",
    "                CASE \n",
    "                    WHEN COUNT(DISTINCT cos.SALES_ORDER_NO) >= 3 \n",
    "                    THEN 1.0 - ABS((\n",
    "                        SUM(CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date THEN cos.items_returned_in_order ELSE 0 END)::FLOAT /\n",
    "                        NULLIF(COUNT(DISTINCT CASE WHEN cos.ORDER_DATE >= rm.recent_cutoff_date THEN cos.SALES_ORDER_NO END), 0)\n",
    "                    ) - (\n",
    "                        SUM(cos.items_returned_in_order)::FLOAT / COUNT(DISTINCT cos.SALES_ORDER_NO)\n",
    "                    )) / NULLIF(\n",
    "                        SUM(cos.items_returned_in_order)::FLOAT / COUNT(DISTINCT cos.SALES_ORDER_NO), 1\n",
    "                    )\n",
    "                    ELSE 0.5\n",
    "                END as BEHAVIOR_STABILITY_SCORE\n",
    "                \n",
    "            FROM customer_base cb\n",
    "            CROSS JOIN reference_metadata rm\n",
    "            LEFT JOIN customer_order_summary cos ON cb.CUSTOMER_EMAILID = cos.CUSTOMER_EMAILID\n",
    "            GROUP BY cb.CUSTOMER_EMAILID, rm.recent_cutoff_date\n",
    "        )\n",
    "        \n",
    "        -- Combine all feature categories\n",
    "        SELECT \n",
    "            bm.CUSTOMER_EMAILID,\n",
    "            \n",
    "            -- Basic Volume Metrics\n",
    "            bm.SALES_ORDER_NO_nunique,\n",
    "            bm.SKU_nunique,\n",
    "            bm.ITEMS_RETURNED_COUNT,\n",
    "            bm.SALES_QTY_mean,\n",
    "            bm.AVG_ORDER_SIZE,\n",
    "            \n",
    "            -- Return Behavior Patterns\n",
    "            rb.RETURN_RATE,\n",
    "            rb.RETURN_RATIO,\n",
    "            rb.RETURN_PRODUCT_VARIETY,\n",
    "            rb.AVG_RETURNS_PER_ORDER,\n",
    "            rb.RETURN_FREQUENCY_RATIO,\n",
    "            rb.RETURN_INTENSITY,\n",
    "            rb.CONSECUTIVE_RETURNS,\n",
    "            rb.AVG_CONSECUTIVE_RETURNS,\n",
    "            \n",
    "            -- Temporal & Timing Patterns\n",
    "            tp.CUSTOMER_LIFETIME_DAYS,\n",
    "            tp.AVG_DAYS_TO_RETURN,\n",
    "            tp.RETURN_TIMING_SPREAD,\n",
    "            tp.CUSTOMER_TENURE_STAGE,\n",
    "            \n",
    "            -- Trend & Recency Analysis\n",
    "            tr.RECENT_ORDERS,\n",
    "            tr.RECENT_RETURNS,\n",
    "            tr.RECENT_VS_AVG_RATIO,\n",
    "            tr.ORDER_FREQUENCY_TREND,\n",
    "            tr.RETURN_FREQUENCY_TREND,\n",
    "            tr.BEHAVIOR_STABILITY_SCORE\n",
    "            \n",
    "            -- TODO: Add when value data available\n",
    "            -- üí∞ MONETARY VALUE PATTERNS\n",
    "            -- mv.AVG_ORDER_VALUE,\n",
    "            -- mv.AVG_RETURN_VALUE,\n",
    "            -- mv.HIGH_VALUE_RETURN_AFFINITY,\n",
    "            \n",
    "            -- üè∑Ô∏è PRODUCT & CATEGORY INTELLIGENCE  \n",
    "            -- pc.PRODUCT_CATEGORY_LOYALTY,\n",
    "            -- pc.CATEGORY_DIVERSITY_SCORE,\n",
    "            -- pc.CATEGORY_LOYALTY_SCORE,\n",
    "            -- pc.HIGH_RETURN_CATEGORY_AFFINITY,\n",
    "            -- pc.HIGH_RISK_PRODUCT_AFFINITY,\n",
    "            -- pc.HIGH_RISK_RETURN_AFFINITY,\n",
    "            \n",
    "            -- üîó ADJACENCY & REPEAT BEHAVIOR\n",
    "            -- ab.SKU_ADJACENCY_ORDERS,\n",
    "            -- ab.SKU_ADJACENCY_RETURNS,\n",
    "            -- ab.SKU_ADJACENCY_TIMING,\n",
    "            -- ab.SKU_ADJACENCY_RETURN_TIMING,\n",
    "            \n",
    "            -- üåä SEASONAL & TREND SUSCEPTIBILITY\n",
    "            -- st.SEASONAL_SUSCEPTIBILITY_RETURNS,\n",
    "            -- st.SEASONAL_SUSCEPTIBILITY_ORDERS,\n",
    "            -- st.TREND_PRODUCT_CATEGORY_RETURN_RATE,\n",
    "            -- st.TREND_SKU_RETURN_RATE,\n",
    "            -- st.TREND_PRODUCT_CATEGORY_ORDER_RATE,\n",
    "            -- st.TREND_SKU_ORDER_RATE\n",
    "            \n",
    "        FROM basic_metrics bm\n",
    "        LEFT JOIN return_behavior rb ON bm.CUSTOMER_EMAILID = rb.CUSTOMER_EMAILID\n",
    "        LEFT JOIN temporal_patterns tp ON bm.CUSTOMER_EMAILID = tp.CUSTOMER_EMAILID\n",
    "        LEFT JOIN trend_recency tr ON bm.CUSTOMER_EMAILID = tr.CUSTOMER_EMAILID\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create index on customer email for performance\n",
    "    conn.execute(f\"CREATE INDEX idx_{features_table_name}_customer ON {features_table_name}(CUSTOMER_EMAILID)\")\n",
    "    \n",
    "    logger.info(f\"Created features table '{features_table_name}' with basic feature set\")\n",
    "    logger.info(\"Advanced features (monetary, category intelligence, adjacency, seasonal) are commented out\")\n",
    "    logger.info(\"Uncomment and implement these sections when additional data becomes available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_features(conn, features_table_name):\n",
    "    \"\"\"Validate feature creation and provide summary statistics.\"\"\"\n",
    "    \n",
    "    # Get basic table info\n",
    "    row_count = conn.execute(f\"SELECT COUNT(*) FROM {features_table_name}\").fetchone()[0]\n",
    "    logger.info(f\"Features table created with {row_count:,} customers\")\n",
    "    \n",
    "    # Get feature summary\n",
    "    feature_summary = conn.execute(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_customers,\n",
    "            AVG(SALES_ORDER_NO_nunique) as avg_orders_per_customer,\n",
    "            AVG(SKU_nunique) as avg_skus_per_customer,\n",
    "            AVG(RETURN_RATE) as avg_return_rate,\n",
    "            COUNT(CASE WHEN ITEMS_RETURNED_COUNT > 0 THEN 1 END) as customers_with_returns,\n",
    "            COUNT(CASE WHEN RECENT_ORDERS > 0 THEN 1 END) as recent_active_customers\n",
    "        FROM {features_table_name}\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    logger.info(f\"Feature Summary:\")\n",
    "    logger.info(f\"  Total customers: {feature_summary[0]:,}\")\n",
    "    logger.info(f\"  Avg orders per customer: {feature_summary[1]:.2f}\")\n",
    "    logger.info(f\"  Avg SKUs per customer: {feature_summary[2]:.2f}\")\n",
    "    logger.info(f\"  Avg return rate: {feature_summary[3]:.2%}\")\n",
    "    logger.info(f\"  Customers with returns: {feature_summary[4]:,} ({feature_summary[4]/feature_summary[0]:.1%})\")\n",
    "    logger.info(f\"  Recently active customers: {feature_summary[5]:,} ({feature_summary[5]/feature_summary[0]:.1%})\")\n",
    "    \n",
    "    # Check for any null values in key features\n",
    "    null_check = conn.execute(f\"\"\"\n",
    "        SELECT \n",
    "            SUM(CASE WHEN SALES_ORDER_NO_nunique IS NULL THEN 1 ELSE 0 END) as null_orders,\n",
    "            SUM(CASE WHEN RETURN_RATE IS NULL THEN 1 ELSE 0 END) as null_return_rate,\n",
    "            SUM(CASE WHEN CUSTOMER_LIFETIME_DAYS IS NULL THEN 1 ELSE 0 END) as null_lifetime\n",
    "        FROM {features_table_name}\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    if any(null_check):\n",
    "        logger.warning(f\"Found null values in key features: {null_check}\")\n",
    "    else:\n",
    "        logger.info(\"No null values found in key features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for complex feature calculations\n",
    "# These would need to be implemented as DuckDB functions or separate queries\n",
    "\n",
    "def _calculate_consecutive_returns(customer_id):\n",
    "    \"\"\"Calculate maximum consecutive orders with returns for a customer.\"\"\"\n",
    "    # TODO: Implement as window function query\n",
    "    return 0\n",
    "\n",
    "def _calculate_avg_consecutive_returns(customer_id):\n",
    "    \"\"\"Calculate average consecutive return streaks for a customer.\"\"\"\n",
    "    # TODO: Implement as window function query\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     # Example: Create features from Excel file\n",
    "#     file_path = \"RETRO_SAMPLE Copy.xlsx\"  # Update with your file path\n",
    "    \n",
    "#     # Create features (adjust chunk_size based on your system)\n",
    "#     conn = create_customer_clustering_features(\n",
    "#         file_path=file_path,\n",
    "#         table_name='customer_transactions',\n",
    "#         features_table_name='customer_clustering_features',\n",
    "#         chunk_size=50000,  # Adjust based on available RAM\n",
    "#         db_file='customer_features.db',  # Use file-based DB for large datasets\n",
    "#         force_recreate=False\n",
    "#     )\n",
    "    \n",
    "#     # View sample of features\n",
    "#     sample_features = conn.execute(\"\"\"\n",
    "#         SELECT * FROM customer_clustering_features \n",
    "#         ORDER BY SALES_ORDER_NO_nunique DESC \n",
    "#         LIMIT 10\n",
    "#     \"\"\").df()\n",
    "    \n",
    "#     print(\"\\nSample Customer Features:\")\n",
    "#     print(sample_features)\n",
    "    \n",
    "#     # Export features for analysis\n",
    "#     # features_df = conn.execute(\"SELECT * FROM customer_clustering_features\").df()\n",
    "#     # features_df.to_csv('customer_clustering_features.csv', index=False)\n",
    "    \n",
    "#     conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
